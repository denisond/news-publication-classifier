{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "# import altair as alt\n",
    "# alt.renderers.enable(\"notebook\")\n",
    "\n",
    "# Code for hiding seaborn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\test\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "------------------------------------------------------------\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\test\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleanish_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication.1</th>\n",
       "      <th>year</th>\n",
       "      <th>content</th>\n",
       "      <th>content_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>29141</td>\n",
       "      <td>GOP House Majority Whip Scalise on Forgoing Au...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>On Sunday’s broadcast of Fox News Channel’s “S...</td>\n",
       "      <td>2034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>38328</td>\n",
       "      <td>Megan Rapinoe Stands for Thailand Anthem Befor...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Megan Rapinoe knelt for “The Star Spangled Ban...</td>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>29992</td>\n",
       "      <td>Trump Administration Solicits Border Wall Prop...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>The Department of Homeland Security wants comp...</td>\n",
       "      <td>2890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>46302</td>\n",
       "      <td>Mark Levin: Trump the Globalist - Breitbart</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Mark Levin writes at Conservative Review:  One...</td>\n",
       "      <td>2150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>26688</td>\n",
       "      <td>Laptop with Trump Tower Floor Plans, National ...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>The New York Daily News cited police sources o...</td>\n",
       "      <td>2699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publication     id                                              title  \\\n",
       "0   Breitbart  29141  GOP House Majority Whip Scalise on Forgoing Au...   \n",
       "1   Breitbart  38328  Megan Rapinoe Stands for Thailand Anthem Befor...   \n",
       "2   Breitbart  29992  Trump Administration Solicits Border Wall Prop...   \n",
       "3   Breitbart  46302        Mark Levin: Trump the Globalist - Breitbart   \n",
       "4   Breitbart  26688  Laptop with Trump Tower Floor Plans, National ...   \n",
       "\n",
       "  publication.1    year                                            content  \\\n",
       "0     Breitbart  2017.0  On Sunday’s broadcast of Fox News Channel’s “S...   \n",
       "1     Breitbart  2016.0  Megan Rapinoe knelt for “The Star Spangled Ban...   \n",
       "2     Breitbart  2017.0  The Department of Homeland Security wants comp...   \n",
       "3     Breitbart  2016.0  Mark Levin writes at Conservative Review:  One...   \n",
       "4     Breitbart  2017.0  The New York Daily News cited police sources o...   \n",
       "\n",
       "   content_len  \n",
       "0         2034  \n",
       "1         2704  \n",
       "2         2890  \n",
       "3         2150  \n",
       "4         2699  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['content_full'] = df['title'] + ' ' + df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['content_cleaned_1'] = df['content_full'].str.replace(\"    \", \" \")\n",
    "df['content_cleaned_2'] = df['content_cleaned_1'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation_signs = list(\"?:!.,;…“”'’\\\"\")\n",
    "df['content_cleaned_3'] = df['content_cleaned_2']\n",
    "\n",
    "for punct_sign in punctuation_signs:\n",
    "    df['content_cleaned_3'] = df['content_cleaned_3'].str.replace(punct_sign, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['content_cleaned_4'] = df['content_cleaned_3'].str.replace(\"'s\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrows = len(df)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['content_cleaned_4']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['content_cleaned_5'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english')) + list(map(lambda x: x.split(' ')[0].lower(), df['publication'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['content_cleaned_6'] = df['content_cleaned_5']\n",
    "for stop_word in stop_words:\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['content_cleaned_6'] = df['content_cleaned_6'].str.replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['content_cleaned_7'] = df['content_cleaned_6']\n",
    "df['content_cleaned_7'] = df['content_cleaned_7'].apply(lambda x: re.sub(\"\\s\\s+\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(1,7):\n",
    "#     print(i, df['Content_Parsed_{}'.format(i)].iloc[5], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pat caddell fuse light political transformation - ask republican presidential candidates sen ted cruz donald trump consider transactional transformational candidates today news daily sirius xm veteran pollster pat caddell tell host stephen k bannon cruzs trump election would shock system [noting trump still way poll add caddell ive say revolutionary period think evidence clear public fact match light public move motion dont know theyre go go ask mean fuse light caddell elaborate see transformation paradigm politics old rule fail old assumptions dont work new set assumptions evolve public longer seem want eat dog food establishment dish fact go choices outside box ordinary politics caddell point conventional wisdom nine months ago jeb bush hillary clinton would prevail respective primaries country say want something different want take back importantly reclaim sovereignty say add people seem go passive observers active participants caddell say believe many people usually didnt vote come also say race south carolina seem get personal may always hard feel impact people ability come toget afterwards always go bloodbath sc politics often poll today correct say caddell hurt trump less faith internal poll push campaign super pacs full interview pollster pat caddell hear news daily air est sirius xm patriot channel 125'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content_cleaned_7'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'content_cleaned_7': 'content_cleaned'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication</th>\n",
       "      <th>content_full</th>\n",
       "      <th>content_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>GOP House Majority Whip Scalise on Forgoing Au...</td>\n",
       "      <td>gop house majority whip scalise forgo august r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Megan Rapinoe Stands for Thailand Anthem Befor...</td>\n",
       "      <td>megan rapinoe stand thailand anthem kneel star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Trump Administration Solicits Border Wall Prop...</td>\n",
       "      <td>trump administration solicit border wall propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Mark Levin: Trump the Globalist - Breitbart Ma...</td>\n",
       "      <td>mark levin trump globalist - mark levin write ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Laptop with Trump Tower Floor Plans, National ...</td>\n",
       "      <td>laptop trump tower floor plan national securit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publication                                       content_full  \\\n",
       "0   Breitbart  GOP House Majority Whip Scalise on Forgoing Au...   \n",
       "1   Breitbart  Megan Rapinoe Stands for Thailand Anthem Befor...   \n",
       "2   Breitbart  Trump Administration Solicits Border Wall Prop...   \n",
       "3   Breitbart  Mark Levin: Trump the Globalist - Breitbart Ma...   \n",
       "4   Breitbart  Laptop with Trump Tower Floor Plans, National ...   \n",
       "\n",
       "                                     content_cleaned  \n",
       "0  gop house majority whip scalise forgo august r...  \n",
       "1  megan rapinoe stand thailand anthem kneel star...  \n",
       "2  trump administration solicit border wall propo...  \n",
       "3  mark levin trump globalist - mark levin write ...  \n",
       "4  laptop trump tower floor plan national securit...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_columns = [\"publication\", \"content_full\", \"content_cleaned\"]\n",
    "df = df[list_columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Breitbart', 'Buzzfeed News', 'CNN', 'Fox News', 'Guardian', 'NPR'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.publication.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'Breitbart': 0,\n",
    "    'Buzzfeed News': 1,\n",
    "    'CNN': 2,\n",
    "    'Fox News': 3, \n",
    "    'Guardian': 4, \n",
    "    'NPR': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Category mapping\n",
    "df['Category_Code'] = df['publication']\n",
    "df = df.replace({'Category_Code':category_codes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication</th>\n",
       "      <th>content_full</th>\n",
       "      <th>content_cleaned</th>\n",
       "      <th>Category_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>GOP House Majority Whip Scalise on Forgoing Au...</td>\n",
       "      <td>gop house majority whip scalise forgo august r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Megan Rapinoe Stands for Thailand Anthem Befor...</td>\n",
       "      <td>megan rapinoe stand thailand anthem kneel star...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Trump Administration Solicits Border Wall Prop...</td>\n",
       "      <td>trump administration solicit border wall propo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Mark Levin: Trump the Globalist - Breitbart Ma...</td>\n",
       "      <td>mark levin trump globalist - mark levin write ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Laptop with Trump Tower Floor Plans, National ...</td>\n",
       "      <td>laptop trump tower floor plan national securit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publication                                       content_full  \\\n",
       "0   Breitbart  GOP House Majority Whip Scalise on Forgoing Au...   \n",
       "1   Breitbart  Megan Rapinoe Stands for Thailand Anthem Befor...   \n",
       "2   Breitbart  Trump Administration Solicits Border Wall Prop...   \n",
       "3   Breitbart  Mark Levin: Trump the Globalist - Breitbart Ma...   \n",
       "4   Breitbart  Laptop with Trump Tower Floor Plans, National ...   \n",
       "\n",
       "                                     content_cleaned  Category_Code  \n",
       "0  gop house majority whip scalise forgo august r...              0  \n",
       "1  megan rapinoe stand thailand anthem kneel star...              0  \n",
       "2  trump administration solicit border wall propo...              0  \n",
       "3  mark levin trump globalist - mark levin write ...              0  \n",
       "4  laptop trump tower floor plan national securit...              0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['content_cleaned'], \n",
    "                                                    df['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5100, 300)\n",
      "(900, 300)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Breitbart' category:\n",
      "  . Most correlated unigrams:\n",
      ". 2016\n",
      ". percent\n",
      ". hillary\n",
      ". follow\n",
      ". twitter\n",
      "  . Most correlated bigrams:\n",
      ". donald trump\n",
      ". hillary clinton\n",
      "\n",
      "# 'Buzzfeed News' category:\n",
      "  . Most correlated unigrams:\n",
      ". clinton\n",
      ". statement\n",
      ". percent\n",
      ". company\n",
      ". news\n",
      "  . Most correlated bigrams:\n",
      ". donald trump\n",
      ". hillary clinton\n",
      "\n",
      "# 'CNN' category:\n",
      "  . Most correlated unigrams:\n",
      ". associate\n",
      ". washington\n",
      ". company\n",
      ". news\n",
      ". percent\n",
      "  . Most correlated bigrams:\n",
      ". unite state\n",
      ". white house\n",
      "\n",
      "# 'Fox News' category:\n",
      "  . Most correlated unigrams:\n",
      ". democratic\n",
      ". associate\n",
      ". cruz\n",
      ". clinton\n",
      ". latest\n",
      "  . Most correlated bigrams:\n",
      ". donald trump\n",
      ". hillary clinton\n",
      "\n",
      "# 'Guardian' category:\n",
      "  . Most correlated unigrams:\n",
      ". clinton\n",
      ". game\n",
      ". percent\n",
      ". film\n",
      ". us\n",
      "  . Most correlated bigrams:\n",
      ". donald trump\n",
      ". hillary clinton\n",
      "\n",
      "# 'NPR' category:\n",
      "  . Most correlated unigrams:\n",
      ". news\n",
      ". clinton\n",
      ". twitter\n",
      ". thats\n",
      ". percent\n",
      "  . Most correlated bigrams:\n",
      ". hillary clinton\n",
      ". donald trump\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5100, 300)\n",
      "(900, 300)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# import seaborn as sns\n",
    "# from matplotlib import pyplot as plt\n",
    "# from scipy import stats\n",
    "# from scipy.stats import mode\n",
    "# from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "# from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#ensembles\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.options.display.max_rows=999\n",
    "pd.options.display.max_columns\n",
    "# from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "# from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "# , train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB: 0.429020 (0.006031)\n",
      "LDA: 0.489020 (0.005764)\n",
      "XGB: 0.498431 (0.009971)\n",
      "ScaledLR: 0.485098 (0.010977)\n",
      "ScaledLDA: 0.489020 (0.005764)\n",
      "RobustScaledGBM: 0.489412 (0.008070)\n"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "seed = 7\n",
    "# scoring = 'accuracy'\n",
    "scoring = 'accuracy'\n",
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('AB', AdaBoostClassifier()))\n",
    "# pipelines.append(('ET', ExtraTreesClassifier()))\n",
    "pipelines.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "pipelines.append(('XGB', XGBClassifier()))\n",
    "pipelines.append(('ScaledLR', \n",
    "                  Pipeline([('Scaler', StandardScaler()),\n",
    "                            ('LR',LogisticRegression())])))\n",
    "pipelines.append(('ScaledLDA', \n",
    "                  Pipeline([('Scaler', StandardScaler()),\n",
    "                            ('LDA', LinearDiscriminantAnalysis())])))\n",
    "# pipelines.append(('ScaledKNN', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('KNN', KNeighborsClassifier())])))\n",
    "\n",
    "pipelines.append(('RobustScaledGBM', \n",
    "                  Pipeline([('RobustScaler', RobustScaler()),\n",
    "                            ('GBM', GradientBoostingClassifier())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, features_train, labels_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_folds = 5\n",
    "# seed = 7\n",
    "# # scoring = 'accuracy'\n",
    "# scoring = 'f1_macro'\n",
    "# # Standardize the dataset\n",
    "# pipelines = []\n",
    "# pipelines.append(('AB', AdaBoostClassifier()))\n",
    "# # pipelines.append(('ET', ExtraTreesClassifier()))\n",
    "# pipelines.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# pipelines.append(('XGB', XGBClassifier()))\n",
    "# pipelines.append(('ScaledLR', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LR',LogisticRegression())])))\n",
    "# pipelines.append(('ScaledLDA', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LDA', LinearDiscriminantAnalysis())])))\n",
    "# # pipelines.append(('ScaledKNN', \n",
    "# #                   Pipeline([('Scaler', StandardScaler()),\n",
    "# #                             ('KNN', KNeighborsClassifier())])))\n",
    "\n",
    "# pipelines.append(('RobustScaledGBM', \n",
    "#                   Pipeline([('RobustScaler', RobustScaler()),\n",
    "#                             ('GBM', GradientBoostingClassifier())])))\n",
    "\n",
    "# results = []\n",
    "# names = []\n",
    "# for name, model in pipelines:\n",
    "#     kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "#     cv_results = cross_val_score(model, features_train, labels_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_folds = 5\n",
    "# seed = 7\n",
    "# # scoring = 'accuracy'\n",
    "# scoring = 'precision'\n",
    "# # Standardize the dataset\n",
    "# pipelines = []\n",
    "# pipelines.append(('AB', AdaBoostClassifier()))\n",
    "# # pipelines.append(('ET', ExtraTreesClassifier()))\n",
    "# pipelines.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# pipelines.append(('XGB', XGBClassifier()))\n",
    "# pipelines.append(('ScaledLR', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LR',LogisticRegression())])))\n",
    "# pipelines.append(('ScaledLDA', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LDA', LinearDiscriminantAnalysis())])))\n",
    "# # pipelines.append(('ScaledKNN', \n",
    "# #                   Pipeline([('Scaler', StandardScaler()),\n",
    "# #                             ('KNN', KNeighborsClassifier())])))\n",
    "\n",
    "# pipelines.append(('RobustScaledGBM', \n",
    "#                   Pipeline([('RobustScaler', RobustScaler()),\n",
    "#                             ('GBM', GradientBoostingClassifier())])))\n",
    "\n",
    "# results = []\n",
    "# names = []\n",
    "# for name, model in pipelines:\n",
    "#     kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "#     cv_results = cross_val_score(model, features_train, labels_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_folds = 5\n",
    "# seed = 7\n",
    "# # scoring = 'accuracy'\n",
    "# scoring = 'recall'\n",
    "# # Standardize the dataset\n",
    "# pipelines = []\n",
    "# pipelines.append(('AB', AdaBoostClassifier()))\n",
    "# # pipelines.append(('ET', ExtraTreesClassifier()))\n",
    "# pipelines.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# pipelines.append(('XGB', XGBClassifier()))\n",
    "# pipelines.append(('ScaledLR', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LR',LogisticRegression())])))\n",
    "# pipelines.append(('ScaledLDA', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LDA', LinearDiscriminantAnalysis())])))\n",
    "# # pipelines.append(('ScaledKNN', \n",
    "# #                   Pipeline([('Scaler', StandardScaler()),\n",
    "# #                             ('KNN', KNeighborsClassifier())])))\n",
    "\n",
    "# pipelines.append(('RobustScaledGBM', \n",
    "#                   Pipeline([('RobustScaler', RobustScaler()),\n",
    "#                             ('GBM', GradientBoostingClassifier())])))\n",
    "\n",
    "# results = []\n",
    "# names = []\n",
    "# for name, model in pipelines:\n",
    "#     kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "#     cv_results = cross_val_score(model, features_train, labels_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "Be sure to add pub name to stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doddo\n"
     ]
    }
   ],
   "source": [
    "print('doddo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
