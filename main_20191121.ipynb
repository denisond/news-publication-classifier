{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "# import altair as alt\n",
    "# alt.renderers.enable(\"notebook\")\n",
    "\n",
    "# Code for hiding seaborn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleanish_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_full'] = df['title'] + ' ' + df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_cleaned_1'] = df['content_full'].str.replace(\"    \", \" \")\n",
    "df['content_cleaned_2'] = df['content_cleaned_1'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_signs = list(\"?:!.,;…“”'’\\\"\")\n",
    "df['content_cleaned_3'] = df['content_cleaned_2']\n",
    "\n",
    "for punct_sign in punctuation_signs:\n",
    "    df['content_cleaned_3'] = df['content_cleaned_3'].str.replace(punct_sign, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_cleaned_4'] = df['content_cleaned_3'].str.replace(\"'s\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(df)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['content_cleaned_4']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_cleaned_5'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english')) + list(map(lambda x: x.split(' ')[0].lower(), df['publication'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_cleaned_6'] = df['content_cleaned_5']\n",
    "for stop_word in stop_words:\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['content_cleaned_6'] = df['content_cleaned_6'].str.replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_cleaned_7'] = df['content_cleaned_6']\n",
    "df['content_cleaned_7'] = df['content_cleaned_7'].apply(lambda x: re.sub(\"\\s\\s+\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,7):\n",
    "#     print(i, df['Content_Parsed_{}'.format(i)].iloc[5], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_cleaned_7'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'content_cleaned_7': 'content_cleaned'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = [\"publication\", \"content_full\", \"content_cleaned\"]\n",
    "df = df[list_columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.publication.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'Breitbart': 0,\n",
    "    'Buzzfeed News': 1,\n",
    "    'CNN': 2,\n",
    "    'Fox News': 3, \n",
    "    'Guardian': 4, \n",
    "    'NPR': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category mapping\n",
    "df['Category_Code'] = df['publication']\n",
    "df = df.replace({'Category_Code':category_codes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['content_cleaned'], \n",
    "                                                    df['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# import seaborn as sns\n",
    "# from matplotlib import pyplot as plt\n",
    "# from scipy import stats\n",
    "# from scipy.stats import mode\n",
    "# from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "# from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#ensembles\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.options.display.max_rows=999\n",
    "pd.options.display.max_columns\n",
    "# from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "# from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "# , train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_folds = 2\n",
    "# seed = 7\n",
    "# # scoring = 'accuracy'\n",
    "# scoring = 'accuracy'\n",
    "# # Standardize the dataset\n",
    "# pipelines = []\n",
    "# pipelines.append(('AB', AdaBoostClassifier()))\n",
    "# # pipelines.append(('ET', ExtraTreesClassifier()))\n",
    "# pipelines.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# pipelines.append(('XGB', XGBClassifier()))\n",
    "# pipelines.append(('ScaledLR', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LR',LogisticRegression())])))\n",
    "# pipelines.append(('ScaledLDA', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LDA', LinearDiscriminantAnalysis())])))\n",
    "# # pipelines.append(('ScaledKNN', \n",
    "# #                   Pipeline([('Scaler', StandardScaler()),\n",
    "# #                             ('KNN', KNeighborsClassifier())])))\n",
    "\n",
    "# pipelines.append(('RobustScaledGBM', \n",
    "#                   Pipeline([('RobustScaler', RobustScaler()),\n",
    "#                             ('GBM', GradientBoostingClassifier())])))\n",
    "\n",
    "# results = []\n",
    "# names = []\n",
    "# for name, model in pipelines:\n",
    "#     kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "#     cv_results = cross_val_score(model, features_train, labels_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_folds = 5\n",
    "# seed = 7\n",
    "# # scoring = 'accuracy'\n",
    "# scoring = 'f1_macro'\n",
    "# # Standardize the dataset\n",
    "# pipelines = []\n",
    "# pipelines.append(('AB', AdaBoostClassifier()))\n",
    "# # pipelines.append(('ET', ExtraTreesClassifier()))\n",
    "# pipelines.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# pipelines.append(('XGB', XGBClassifier()))\n",
    "# pipelines.append(('ScaledLR', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LR',LogisticRegression())])))\n",
    "# pipelines.append(('ScaledLDA', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LDA', LinearDiscriminantAnalysis())])))\n",
    "# # pipelines.append(('ScaledKNN', \n",
    "# #                   Pipeline([('Scaler', StandardScaler()),\n",
    "# #                             ('KNN', KNeighborsClassifier())])))\n",
    "\n",
    "# pipelines.append(('RobustScaledGBM', \n",
    "#                   Pipeline([('RobustScaler', RobustScaler()),\n",
    "#                             ('GBM', GradientBoostingClassifier())])))\n",
    "\n",
    "# results = []\n",
    "# names = []\n",
    "# for name, model in pipelines:\n",
    "#     kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "#     cv_results = cross_val_score(model, features_train, labels_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_folds = 5\n",
    "# seed = 7\n",
    "# # scoring = 'accuracy'\n",
    "# scoring = 'precision'\n",
    "# # Standardize the dataset\n",
    "# pipelines = []\n",
    "# pipelines.append(('AB', AdaBoostClassifier()))\n",
    "# # pipelines.append(('ET', ExtraTreesClassifier()))\n",
    "# pipelines.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# pipelines.append(('XGB', XGBClassifier()))\n",
    "# pipelines.append(('ScaledLR', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LR',LogisticRegression())])))\n",
    "# pipelines.append(('ScaledLDA', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LDA', LinearDiscriminantAnalysis())])))\n",
    "# # pipelines.append(('ScaledKNN', \n",
    "# #                   Pipeline([('Scaler', StandardScaler()),\n",
    "# #                             ('KNN', KNeighborsClassifier())])))\n",
    "\n",
    "# pipelines.append(('RobustScaledGBM', \n",
    "#                   Pipeline([('RobustScaler', RobustScaler()),\n",
    "#                             ('GBM', GradientBoostingClassifier())])))\n",
    "\n",
    "# results = []\n",
    "# names = []\n",
    "# for name, model in pipelines:\n",
    "#     kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "#     cv_results = cross_val_score(model, features_train, labels_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_folds = 5\n",
    "# seed = 7\n",
    "# # scoring = 'accuracy'\n",
    "# scoring = 'recall'\n",
    "# # Standardize the dataset\n",
    "# pipelines = []\n",
    "# pipelines.append(('AB', AdaBoostClassifier()))\n",
    "# # pipelines.append(('ET', ExtraTreesClassifier()))\n",
    "# pipelines.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# pipelines.append(('XGB', XGBClassifier()))\n",
    "# pipelines.append(('ScaledLR', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LR',LogisticRegression())])))\n",
    "# pipelines.append(('ScaledLDA', \n",
    "#                   Pipeline([('Scaler', StandardScaler()),\n",
    "#                             ('LDA', LinearDiscriminantAnalysis())])))\n",
    "# # pipelines.append(('ScaledKNN', \n",
    "# #                   Pipeline([('Scaler', StandardScaler()),\n",
    "# #                             ('KNN', KNeighborsClassifier())])))\n",
    "\n",
    "# pipelines.append(('RobustScaledGBM', \n",
    "#                   Pipeline([('RobustScaler', RobustScaler()),\n",
    "#                             ('GBM', GradientBoostingClassifier())])))\n",
    "\n",
    "# results = []\n",
    "# names = []\n",
    "# for name, model in pipelines:\n",
    "#     kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "#     cv_results = cross_val_score(model, features_train, labels_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "Be sure to add pub name to stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('doddo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
